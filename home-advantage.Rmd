---
title: "Home Advantage Literature Synthesis"
author: "Gjalt-Jorn Peters & Tim Vincken"
date: "`r format(Sys.time(), '%H:%M:%S on %Y-%m-%d %Z (GMT%z)')`"
output:
  html_document:
    df_print: paged
---

```{r setup-chunk, include=FALSE}
########################################################################
### Load packages
########################################################################

require('userfriendlyscience');  ### For convenience functions, e.g. 'safeRequire'
safeRequire('here');             ### To easily access files using 'relative paths'
safeRequire('plyr');             ### For easily processing and restructuring data
safeRequire("googlesheets");     ### To import data from google sheets in metabefor
safeRequire('jsonlite');         ### To import a list of country codes in metabefor
safeRequire('readxl');           ### To read Excel files
safeRequire('knitr');            ### For kable
safeRequire('grid');             ### Force drawing of plots
safeRequire('data.tree');        ### To work with data structured in a tree in metabefor
safeRequire('metafor');          ### For the meta-analysis
safeRequire('ggplot2');          ### For saving plots
safeRequire('devtools');         ### To install metabefor from github repo
                                 ### ... Which we then do here:
devtools::install_github("Matherion/metabefor");
require('metabefor');

########################################################################
### Settings
########################################################################

### By default hide R code
knitr::opts_chunk$set(echo = FALSE);

### Set path for query hit exports
queryHitExportPath <- here::here("queries");

### Set path for screening
screeningPath <- here::here("screening");

### Set path for extraction script template
extractionScriptTemplatePath <- here::here("extraction");
extractionScriptsPath <- extractionScriptTemplatePath;

### Working path
workingPath <- here::here("output");

```

## First query, executed at: 2018-05-14

The first query is:

```
("Home advantage" OR "home court advantage" OR "home team advantage" OR "home venue advantage" OR "home field advantage") AND ("soccer" OR "football") NOT ("American Football" NOT "Australian football") 
```

This was run at 2018-05-14 in PubMed (53 hits) and PsycINFO accessed through EbscoHost (63 hits), and exported to csv, RIS (called 'medline' in PubMed) and bibtex (from PsycINFO) formats. The RIS files were then imported in R using `metabefor`.

```{r echo=TRUE}

### Import PsycINFO hits
firstQueryIteration_psycinfo <-
  importRISlike(file.path(queryHitExportPath,
                          "psycinfo-2018-05-14.ris"),
                encoding="native.enc");

### Import PubMed hits
firstQueryIteration_pubmed <-
  importRISlike(file.path(queryHitExportPath,
                          "pubmed-2018-05-14.ris"));

### Merge the two sets of hits
firstQueryIteration <-
  findDuplicateReferences(primaryRefs = firstQueryIteration_psycinfo,
                          secondaryRefs = firstQueryIteration_pubmed,
                          duplicateFieldValue = "dupl",
                          newRecordValue = "PubMed",
                          duplicateValue = "duplicate (both PsycINFO and PubMed)",
                          originalValue = "PsycINFO");

### Generate bibtex keys
firstQueryIteration$output$records <-
  generateBibtexkeys(firstQueryIteration$output$records);

### Add query date identifier to bibtex keys
firstQueryIteration$output$records$bibtexkey <-
  paste0(firstQueryIteration$output$records$bibtexkey,
         "-20180514");

### Export the hits to bibtex for screening in JabRef
sysrevExport(firstQueryIteration,
             filename=file.path(screeningPath,
                                "2018-05-14-screening.bib"),
             screeningType="screening");

```

The merged list of query hits has now been exported to file `2018-05-14-screening.bib` in directory "screening" and can be opened using JabRef, which can be downloaded from https://www.fosshub.com/JabRef.html.

## JabRef configuration

When opening a bibliographic library (i.e. a file with the extension `.bib`) in JabRef, it will show the entry table, which is a convenient way to inspect all entries (hits, references, articles, etc) in the library. To prepare JabRef for screening, two settings are important.

First, to change the fields that are visible in the overview table of all references (i.e. the entry table), open the 'Options' drop-down menu and select 'Preferences'. In the preferences dialog, open the 'Entry table columns' section:

![Figure 1: Screenshot of JabRef preferences dialog when the 'Entry table columns' section is opened.](img/jabref--preferences--entry-table-columns.png)

There, the columns shown in the entry table can be edited in the 'Entry table columns' sections. A bit confusingly, this is done by adding *rows* in the table shown in this dialog. Each 'row' in this table represents a column in the entry table.

Note that in bibtex (and therefore JabRef), you can create new fields on the fly. In this case, use field 'screening1' for screening the hits of this first screening iteration: simply add this field name as a 'row' (column) in the entry table. This will show, for every entry, the contents of that field (if it has any).

Second, you need to be able to edit the content in that field. The entry table is very convenient to maintain an overview of the entries in the database, but cannot be used for editing. To edit an entry, double click it in the entry tabel. This opens the entry editor, which has a number of tabs. Each tab shows a number of fields which can then be edited.

These tabs can be configured by setting the 'General fields'. Open the 'Options' drop-down menu and select 'General Fields' to configure which fields are available in the different tabs when opening an entry. 

![Figure 2: Screenshot of JabRef dialog used to set the general fields.](img/jabref--set-general-fields.png)

Add a dedicated field for the reviewing, showing only the title, abstract, and `screening1` fields. This allows you to focus on the relevant information while ignoring irrelevant and potentially biasing information (such as year, journal, and authors). Each row in this text area shows one tab. The first term on each row is the tab's name, followed by a colon (`:`) and then the fields shown in the tab, separated by semicolons (`;`). For example, you could add the following row:

`Screening Round 1:title;abstract;screening1`

## Screening process

For every entry, add the following text in the 'screening' field:

- If it is excluded, add the reason, specifically (these are ordered progressively; i.e. if one of the criteria matches, apply it and move on to the next entry):
    - **`nohome`** if the study does not deal with the home advantage;
    - **`nosoccer`** if the study does not report on data exclusively for soccer (*also* reporting data for other sports is no grounds for exclusion of course)
    - **`nopro`** if the study does not report on data exclusively for professionals (*also* reporting data for amateur soccer is no grounds for exclusion of course)
- If it is included, add **`incl`**, followed by two opening brackets (**`[[`**) and a list of the relevant variables that were included separated by pipes (**`|`**) and terminated by two closing brackets (**`]]`**), such as `incl [[ supporting audience size | distance from home city ]]`

So once JabRef is opened, when screening, make sure that the 'screening1' field is shown in the entry table (i.e. that it is one of the entry table columns), and create one entry editing tab using 'General Fields' that contains the fields `title`, `abstract`, and `screening1`. You can then use this tab for the screening. It is also convenient to show field `dupl` in either the entry table or the screening tab in the entry editor, because for duplicate records (that were identified as such - the algorithm may miss some duplicates of course), that field contains the text `dupl`.

Make sure to save the database with query hits under a different name than "2018-05-14-screening.bib". That is important because file "2018-05-14-screening.bib" will get overwritten if this R Markdown file is executed again.

## Second query, executed at: 2018-05-23

The query was updated to:

```
("Home advantage" OR "home court advantage" OR "home team advantage" OR "home venue advantage" OR "home field advantage" OR "game location") AND ("soccer" OR "football") NOT ("American Football" OR "Australian football") 
```

This was run at 2018-05-23 in PubMed (61 hits) and PsycINFO accessed through EbscoHost (72 hits), and exported to the RIS format (called 'medline' in PubMed). The RIS files were then imported in R using `metabefor`.

```{r echo=TRUE}

### Import PsycINFO hits
secondQueryIteration_psycinfo <-
  importRISlike(file.path(queryHitExportPath,
                          "psycinfo-2018-05-23.ris"),
                encoding="native.enc");

### Import PubMed hits
secondQueryIteration_pubmed <-
  importRISlike(file.path(queryHitExportPath,
                          "pubmed-2018-05-23.ris"));

### Merge the two sets of hits
secondQueryIteration <-
  findDuplicateReferences(primaryRefs = secondQueryIteration_psycinfo,
                          secondaryRefs = secondQueryIteration_pubmed,
                          duplicateFieldValue = "dupl (2nd)",
                          newRecordValue = "PubMed (2nd)",
                          duplicateValue = "duplicate (both PsycINFO and PubMed; 2nd)",
                          originalValue = "PsycINFO (2nd)");

### Generate bibtex keys
secondQueryIteration$output$records <-
  generateBibtexkeys(secondQueryIteration$output$records);

### Add query date identifier to bibtex keys
secondQueryIteration$output$records$bibtexkey <-
  paste0(secondQueryIteration$output$records$bibtexkey,
         "-20180523");

### Import results from first query (these have been screened now)
firstQueryIteration_screened <-
  importBibtex(file.path(screeningPath,
                         "2018-05-14-screening#1.bib"));

### Merge the screened reference database from the first query
### with the database from the second query
secondQueryIteration_merged <-
  findDuplicateReferences(primaryRefs = firstQueryIteration_screened,
                          secondaryRefs = secondQueryIteration,
                          duplicateFieldValue = "Screened in first iteration",
                          newRecordValue = "From second query",
                          duplicateValue = "From first query (screened in first iteration)",
                          originalValue = "screening1");

### The new records are stored in secondQueryIteration_merged$output$newRecords, so we
### can copy these to the database from the first screening. We also store the entire
### database so that we can document the process (and if need be, check whether anything
### went wrong).

secondScreening <- firstQueryIteration_screened;

secondScreening$output$records <- rbind.fill(secondScreening$output$records,
                                             secondQueryIteration_merged$output$newRecords);

### Export the hits to bibtex for screening in JabRef
sysrevExport(secondQueryIteration_merged,
             filename=file.path(screeningPath,
                                "2018-05-23-fully-merged-database.bib"),
             screeningType="screening");

sysrevExport(secondScreening,
             filename=file.path(screeningPath,
                                "2018-05-23-screening.bib"),
             screeningType="screening");

```

The merged bibliographic database has been stored in the screening path (`r screeningPath`), to file `2018-05-23-screening.bib`. This file can now be opened in JabRef, and should be saved to a different filename before any edits are made (because, after all, the file named `2018-05-23-screening.bib` will be overwritten every time this script runs).

In this merged database, field 'screening1' has been preserved. Records where this field is empty, therefore, are from the second query, and should be screened in the second screening sweep.

## Generation of the Extraction Script

We will use a metabefor extraction script for the extraction of the data. The idea of this script is to extract the data from the original sources with a minimum of interpretation. The data is extracted into a machine-readable format, which then allows competely transparent further processing and synthesis.

These scripts are generated on the basis of two tables/spreadsheets. The first contains the entities to extract, such as study year, sample size, how variables were operationalised, and associations that were found. The second contains the valid values for each entity, to allow efficiently providing coders with examples, instructions, and to allow easy verification of the input.

```{r extraction-script-generation}

# sheetsURL <- paste0("https://docs.google.com/spreadsheets/d/",
#                     "1S14aH6ng7_e9wkNTT4zJZCiWNhAbk5vGLM78OHdFPYE");
# 
# valueTemplatesSheet <- "valueTemplates";
# entitiesSheet <- "entities";
# 
# rxs_fromSpecifications(gs_url = sheetsURL,
#                        entitiesFilename = file.path(extractionScriptTemplatePath,
#                                                     "entities-local-copy.csv"),
#                        valueTemplatesFilename = file.path(extractionScriptTemplatePath,
#                                                           "valueTemplates-local-copy.csv"),
#                        localBackup = list(entities = file.path(extractionScriptTemplatePath,
#                                                                "entities-local-copy.csv"),
#                                           valueTemplates= file.path(extractionScriptTemplatePath,
#                                                                     "valueTemplates-local-copy.csv"),
#                                           definitions = NULL),
#                        outputFile = file.path(extractionScriptTemplatePath,
#                                               "extractionScriptTemplate.rxs.Rmd"));

```

The completed extraction scripts are then read into R.

```{r extraction-script-parsing, results="asis", messages=FALSE, warning=FALSE}

rxs <- rxs_parseExtractionScripts(extractionScriptsPath);

names(rxs$rxsTrees) <-
  gsub("(.*).rxs.Rmd.*", "\\1", names(rxs$rxsTrees));

cat("\n\n# Extraction script output {.tabset}\n\n");

for (currentFile in names(rxs$rxsOutput)) {
  cat("\n\n## ", currentFile, "\n\n");
  cat0(rxs$rxsOutput[[currentFile]], sep="\n");
}

### Get list of opeationalisations
operationalisations <- rxs_get_values(rxs,
                                      'oper.name');

operationalisations <- data.frame(study = names(operationalisations),
                                  operationalisation = operationalisations);

operationalisations$study <-
  gsub("(.*).rxs.Rmd.*", "\\1", operationalisations$study);

row.names(operationalisations) <- NULL;

write.csv(operationalisations,
          file.path(workingPath, "operationalisations.csv"));

```

# Results

## A full list of all extracted entities 

```{r all-extracted-entities}

rxs_extractedListEntities(rxs);

```

## The full list of extracted operationalisations {.tabset}

### Description

In total, `r nrow(operationalisations)` operationalisations were extracted. View the second tab in this section to see them all.

### Full list

```{r operationalisations-original}
operationalisations;
```

## Merged operationalisations

These operationalisations were then aggregated into overarching categories.

```{r operationalisations-categorized}

operationalisations_categorized <-
  read_excel(file.path(workingPath, "operationalisations_scriptie_excel.xlsx"),
             sheet=2, skip=1);

operationalisations_categorized <-
  as.data.frame(operationalisations_categorized[, -1]);

names(operationalisations_categorized) <- c('study', 'operationalisations', 'category1', 'category2');

operationalisations_categorized;

```

## Sample size copying

The sample sizes were then copied to each association.

```{r copy-sample-sizes, results='asis'}

invisible(lapply(names(rxs$rxsTrees), function(studyName) {
  rxs$rxsTrees[[studyName]]$Do(function(nd) {
    ### Then check which value for n we will use
    if (is.null(nd$value$assoc.n) || is.na(nd$value$assoc.n)) {
      cat0("\n- In study '", studyName,
           "', no sample size specified in association '", nd$name, "'; ");
      if (is.null(nd$value$assoc.subsample) || is.na(nd$value$assoc.subsample)) {
        ### Use general study n
        nd$value$assoc.n <- nd$root$methods$N$value;
        cat0("added sample size from study metadata (", nd$value$assoc.n, ").");
      } else {
        ### Use n from a subsample
        useNode <- FindNode(node=nd$root,
                            name=nd$value$assoc.subsample);
        nd$value$assoc.n <-
          useNode$value$subsample.N;
        cat0("added sample size from metadata from subsample '",
            nd$value$assoc.subsample,
            "' (", nd$value$assoc.n, ").");
        if (!is.null(useNode$value$subsample.Ntype) && 
            !is.na(useNode$value$subsample.Ntype) &&
            useNode$value$subsample.Ntype == "season") {
          nd$value$assoc.n <- 34 * nd$value$assoc.n;
        } else if(!is.null(useNode$value$subsample.Ntype) && 
                  !is.na(useNode$value$subsample.Ntype) &&
                  useNode$value$subsample.Ntype == "team") {
          nd$value$assoc.n <- 1 * nd$value$assoc.n;
        }
      }
    }
  }, filterFun=function(nd) {
    ### Only get lists from the results
    return(('results' %IN% nd$path) &&
           ('value' %IN% nd$fields) &&
           (is.list(nd$value)));
  });
}));

```

## Effect size aggregation

The effect sizes were then aggregated. Within the results entity, the following entities were extracted (and with this frequency):

```{r all-extracted-effect-sizes}
count(rxs_extractedListEntities(rxs, withinEntity='results')$entity);
```

Thus, of all effect sizes, it seems like the correlation coefficient was the most common. We will therefore choose the correlation coefficient as basic effect size, and convert all other effect sizes to Pearson's *r*.

```{r aggregate-effect-sizes}

### First, we process the Cohen's d values using convert.d.to.r
### We will assume we have only one group size for now, and only
### specify n1
invisible(lapply(rxs$rxsTrees, function(node) {
  node$Do(function(nd) {
    ### First check whether r is missing but d is present
    if ((is.null(nd$value$assoc.r) || is.na(nd$value$assoc.r)) &&
        (!is.null(nd$value$assoc.d) && !is.na(nd$value$assoc.d))) {
      ### Then check which value for n we will use
      if (is.null(nd$value$assoc.n) || is.na(nd$value$assoc.n)) {
        if (is.null(nd$value$assoc.subsample) || is.na(nd$value$assoc.subsample)) {
          ### Use general study n
          n <-
            nd$root$methods$N$value;
        } else {
          ### Use n from a subsample
          n <-
            FindNode(node=node$root,
                     name=nd$value$assoc.subsample)$value$subsample.N;
        }
      } else {
        ### Use n specified for this analysis
        n <- nd$value$assoc.n;
      }
      ### Convert d to r and store it
      nd$value$assoc.r <- convert.d.to.r(d = nd$value$assoc.d,
                                         n1 = n);
    }
  }, filterFun=function(nd) {
    ### Only get lists from the results
    return(('results' %IN% nd$path) &&
           ('value' %IN% nd$fields) &&
           (is.list(nd$value)));
  });
}));


### A Generalized Formula for Converting
### Chi-Square Tests to Effect Sizes for Meta-Analysis
### http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0010059


### https://www.personality-project.org/r/html/phi2poly.html

### Next up: converting chi squared to V (which is basically r)
# invisible(lapply(rxs$rxsTrees, function(node) {
#   node$Do(function(nd) {
#     ### First check whether r is missing but d is present
#     if ((is.null(nd$value$assoc.r) || is.na(nd$value$assoc.r)) &&
#         (!is.null(nd$value$assoc.V) && !is.na(nd$value$assoc.V))) {
#       ### Convert V to r and store it
#       
#       
#       
#       
#       
#       
#       nd$value$assoc.r <- convert.V.to.r(V = nd$value$assoc.V);
#     }
#   }, filterFun=function(nd) {
#     ### Only get lists from the results
#     return(('results' %IN% nd$path) &&
#            ('value' %IN% nd$fields) &&
#            (is.list(nd$value)));
#   });
# }));

### Next up: converting eta squared (which is basically r)
invisible(lapply(rxs$rxsTrees, function(node) {
  node$Do(function(nd) {
    ### First check whether r is missing but d is present
    if ((is.null(nd$value$assoc.r) || is.na(nd$value$assoc.r)) &&
        (!is.null(nd$value$assoc.etasq) && !is.na(nd$value$assoc.etasq))) {
      ### Convert V to r and store it
      nd$value$assoc.r <- convert.etasq.to.r(etasq = nd$value$assoc.etasq);
    }
  }, filterFun=function(nd) {
    ### Only get lists from the results
    return(('results' %IN% nd$path) &&
           ('value' %IN% nd$fields) &&
           (is.list(nd$value)));
  });
}));


```

## Adding operationalisation categorisation to the study tree

```{r adding-operationalisation-categorisation}

invisible(apply(operationalisations_categorized,
                1,
                function(cat_oper) {
      rxs$rxsTrees[[cat_oper['study']]]$results$Do(function(node) {
        if (!is.null(node$value$assoc.var1name) &&
            !is.na(node$value$assoc.var1name) &&
            (node$value$assoc.var1name == cat_oper['operationalisations'])) {
          node$value$assoc.var1_cat1 <- cat_oper['category1'];
          node$value$assoc.var1_cat2 <- cat_oper['category2'];
          cat0("\nIn study '", cat_oper['study'],
               "' added categorisation to '",
               node$value$assoc.var1name, "'.");
        } else if (!is.null(node$value$assoc.var2name) &&
            !is.na(node$value$assoc.var2name) &&
            (node$value$assoc.var2name == cat_oper['operationalisations'])) {
          node$value$assoc.var2_cat1 <- cat_oper['category1'];
          node$value$assoc.var2_cat2 <- cat_oper['category2'];
          cat0("\nIn study '", cat_oper['study'],
               "' added categorisation to '",
               node$value$assoc.var2name, "'.");
        }
      });
    }));

cat("\n\n");

```

## Get dataframe with effect sizes, studies, and variables

```{r result-dataframe, cols.print=14}

dat <-
  lapply(names(rxs$rxsTrees),
         function(studyName) {
           rxs$rxsTrees[[studyName]]$Get(function(node) {
               if (!is.null(node$value$assoc.var1name) &&
                   !is.na(node$value$assoc.var1name) &&
                   !is.null(node$value$assoc.var2name) &&
                   !is.na(node$value$assoc.var2name)) {
                 resDf <- data.frame(study=studyName,
                                   var1 = node$value$assoc.var1name,
                                   var2 = node$value$assoc.var2name);
                 resDf[, 'var1_cat1'] <- ifelse(
                   is.null(node$value$assoc.var1_cat1),
                           NA,
                           node$value$assoc.var1_cat1);
                 resDf[, 'var1_cat2'] <- ifelse(
                   is.null(node$value$assoc.var1_cat2),
                           NA,
                           node$value$assoc.var1_cat2);
                 resDf[, 'var2_cat1'] <- ifelse(
                   is.null(node$value$assoc.var2_cat1),
                           NA,
                           node$value$assoc.var2_cat1);
                 resDf[, 'var2_cat2'] <- ifelse(
                   is.null(node$value$assoc.var2_cat2),
                           NA,
                           node$value$assoc.var2_cat2);
                 resDf[, 'r'] <- ifelse(
                   is.null(node$value$assoc.r),
                           NA,
                           node$value$assoc.r);
                 resDf[, 'n'] <- ifelse(
                   is.null(node$value$assoc.n),
                           NA,
                           node$value$assoc.n);
                 resDf <- resDf[, c('study',
                                    'r',
                                    'var1',
                                    'var2',
                                    'var1_cat1',
                                    'var1_cat2',
                                    'var2_cat1',
                                    'var2_cat2',
                                    'n')];
                 return(resDf);
               }
             }, filterFun=function(nd) {
               return(('results' %IN% nd$path) &&
                      ('value' %IN% nd$fields) &&
                      (is.list(nd$value)));
               });
         });

dat <- do.call(rbind, unlist(dat, recursive=FALSE));

row.names(dat) <- NULL;

dat;

### Add variances

dat[, c('yi', 'vi')] <- escalc('COR', ri=dat$r, ni=as.numeric(dat$n));

completeDat <- dat[!is.na(dat$vi), ];

```

## Diamond plot of all effect sizes

```{r diamond-plots, fig.height=14, warning=FALSE}

dat[, c('yi', 'vi')] <- escalc('COR', ri=dat$r, ni=as.numeric(dat$n));

completeDat <- dat[!is.na(dat$vi), ];

completeDat$var1_cat1 <- ifelse(is.na(completeDat$var1_cat1),
                                "uncategorized",
                                completeDat$var1_cat1);
completeDat$var2_cat1 <- ifelse(is.na(completeDat$var2_cat1),
                                "uncategorized",
                                completeDat$var2_cat1);

depVars <- c('HAoperationalisatie',
             'HAuitkomst');

completeDat$iv <- ifelse(completeDat$var1_cat1 %in% depVars,
                         completeDat$var2_cat1,
                         completeDat$var1_cat1);

### Flip outcome if need be; normally, var2 is the dependent variable,
### but if var1 is the dependent variable, the outcomes are reverse coded
completeDat$r <- ifelse(completeDat$var1_cat1 %in% depVars,
                        -1 * completeDat$r,
                        completeDat$r);

### Add confidence intervals
completeDat[, c('lo', 'hi')] <- t(mapply(confIntR, completeDat$r, as.numeric(completeDat$n)));

### Split dataframe for separate diamond plots
completeDat_locationEffects <- completeDat[completeDat$var1_cat1=="HAoperationalisatie" |
                                           completeDat$var2_cat1=="HAoperationalisatie", ];

completeDat_homeAdvantage <- completeDat[completeDat$var1_cat1=="HAuitkomst" |
                                         completeDat$var2_cat1=="HAuitkomst", ];

rawDiamondPlot <-
  diamondPlot(completeDat[, c('lo', 'r', 'hi')],
              yLabels=paste(completeDat$study, "-", completeDat$var1, "&", completeDat$var2));
rawDiamondPlot;
ggsave(file.path(workingPath, "raw-diamond-plot-without-meta-analysis.png"),
       height=14, width=8);
```

### Game location, before meta-analysis

```{r diamond-plot-location, fig.height=10}

locationOnly <-
  diamondPlot(completeDat_locationEffects[, c('lo', 'r', 'hi')],
              yLabels=paste(completeDat_locationEffects$study, "-", completeDat_locationEffects$var1, "&", completeDat_locationEffects$var2));
locationOnly;
ggsave(file.path(workingPath, "diamond-plot-for-location-without-meta-analysis.png"),
       height=10, width=8);

kable(completeDat_locationEffects[, c('study', 'lo', 'r', 'hi')]);

```

### Home advantage, before meta-analysis

```{r diamond-plot-home-advantage}

homeAdvantage <-
  diamondPlot(completeDat_homeAdvantage[, c('lo', 'r', 'hi')],
              yLabels=paste(completeDat_homeAdvantage$study, "-", completeDat_homeAdvantage$var1, "&", completeDat_homeAdvantage$var2));
homeAdvantage;
ggsave(file.path(workingPath, "diamond-plot-for-home-advantage-without-meta-analysis.png"),
       height=8, width=8);

kable(completeDat_homeAdvantage[, c('study', 'lo', 'r', 'hi')]);

```

### Meta-analysis of game location effect sizes {.tabset}

#### Main results

```{r game-location-ma-overview}

gameLocationRma <- dlply(completeDat_locationEffects, .(iv),
                         function(x) {
                           return(rma(x$yi, x$vi));
                         });
gameLocationDf <- do.call(rbind,
                          lapply(gameLocationRma,
                                 function(rmaRes) {
                                   return(data.frame(lo = rmaRes$ci.lb,
                                                     r = rmaRes$b,
                                                     hi = rmaRes$ci.ub));
                                 }));
gameLocationDf$vars <- names(gameLocationRma);
gameLocationDf <- gameLocationDf[gameLocationDf$vars != 'uncategorized', ];

locationRmaDiamondplot <-
  diamondPlot(gameLocationDf[, c('lo', 'r', 'hi')], yLabels=gameLocationDf$vars);
locationRmaDiamondplot;
ggsave(file.path(workingPath, "diamond-plot-for-location-with-meta-analysis.png"),
       height=8, width=8);

kable(gameLocationDf[, c('vars', 'lo', 'r', 'hi')]);

```

#### Detailed meta-analysis

```{r game-location-ma-details, results='asis'}

invisible(mapply(function(xRma, xName) {
    cat0("\n\n##### ", xName, "\n\n");
    cat0("\n\n");
    cat0(paste0("<pre>",
                paste0(capture.output(xRma), collapse="\n"),
                "</pre>"));
    forest(xRma);
    cat0("\n\n");
    cat0("\n\n");
  },
  gameLocationRma,
  names(gameLocationRma)));

```

### Meta-analysis of home advantage effect sizes {.tabset}

#### Main results

```{r home-advantage-ma}

homeAdvantageRma <- dlply(completeDat_homeAdvantage, .(iv),
                          function(x) {
                            return(rma(x$yi, x$vi));
                          });
homeAdvantageDf <- do.call(rbind,
                           lapply(homeAdvantageRma,
                                  function(rmaRes) {
                                    return(data.frame(lo = rmaRes$ci.lb,
                                                      r = rmaRes$b,
                                                      hi = rmaRes$ci.ub));
                                  }));
homeAdvantageDf$vars <- names(homeAdvantageRma);
homeAdvantageDf <- homeAdvantageDf[homeAdvantageDf$vars != 'uncategorized', ];

homeAdvantageRmaDiamondplot <-
  diamondPlot(homeAdvantageDf[, c('lo', 'r', 'hi')], yLabels=homeAdvantageDf$vars);
homeAdvantageRmaDiamondplot;
ggsave(file.path(workingPath, "diamond-plot-for-home-advantage-with-meta-analysis.png"),
       height=8, width=8);

kable(homeAdvantageDf[, c('vars', 'lo', 'r', 'hi')]);

```

#### Detailed meta-analysis

```{r home-advantage-ma-details, results='asis'}

invisible(mapply(function(xRma, xName) {
    cat0("\n\n##### ", xName, "\n\n");
    cat0("\n\n");
    cat0(paste0("<pre>",
                paste0(capture.output(xRma), collapse="\n"),
                "</pre>"));
    cat0("\n\n");
    forest(xRma);
    cat0("\n\n");
  },
  homeAdvantageRma,
  names(homeAdvantageRma)));

```
